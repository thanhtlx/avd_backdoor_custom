{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer(\n",
    "        \".+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)\",\n",
    "        identifier,\n",
    "    )\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "def special_split(token):\n",
    "    #!@#$%^&*()-+?_=,<>/|\n",
    "    token = re.sub(r\"([!@#$%^&*()-+?_=,<>/\\.\\[\\]\\:])+\", r\" \\1 \", token)\n",
    "    return token.split()\n",
    "\n",
    "def subtokens(in_list):\n",
    "    good_list = []\n",
    "    for tok in in_list:\n",
    "        for subtok in tok.replace(\"_\", \" \").split(\" \"):\n",
    "            if subtok.strip() != \"\":\n",
    "                for subsubtok in special_split(subtok):\n",
    "                    good_list.extend(camel_case_split(subsubtok))\n",
    "\n",
    "    return good_list\n",
    "\n",
    "\n",
    "def clean_name(in_list):\n",
    "    return subtokens(in_list)\n",
    "\n",
    "\n",
    "def normalize_subtoken(subtoken):\n",
    "    normalized = re.sub(\n",
    "        r\"[^\\x00-\\x7f]\",\n",
    "        r\"\",  # Get rid of non-ascii\n",
    "        re.sub(\n",
    "            r'[\"\\'`]',\n",
    "            r\"\",  # Get rid of quotes and comma\n",
    "            re.sub(\n",
    "                r\"\\s+\",\n",
    "                r\"\",  # Get rid of spaces\n",
    "                subtoken.replace(\"\\\\\\n\", \"\")\n",
    "                .replace(\"\\\\\\t\", \"\")\n",
    "                .replace(\"\\\\\\r\", \"\"),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return normalized.strip()\n",
    "\n",
    "\n",
    "def tokenizer_code(code):\n",
    "    code_tokens = list(\n",
    "        filter(None, [normalize_subtoken(subtok) for subtok in subtokens(code.split())])\n",
    "    )\n",
    "    return code_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "def :C:NN2d:(CNN=None, second=10, saveable=True, name='cnn', fig_idx=3119362):\\n    import matplotlib.pyplot as plt\\n    # tl.logging.info(CNN.shape)    # (5, 5, 3, 64)\\n    # exit()\\n    n_mask = CNN.shape[3]\\n    n_row = CNN.shape[0]\\n    n_col = CNN.shape[1]\\n    n_color = CNN.shape[2]\\n    row = int(np.sqrt(n_mask))\\n    col = int(np.ceil(n_mask / row))\\n    plt.ion()  # active mode\\n    fig = plt.figure(fig_idx)\\n    count = 1\\n    for _ir in range(1, row + 1):\\n        for _ic in range(1, col + 1):\\n            if count > n_mask:\\n                break\\n            fig.add_subplot(col, row, count)\\n            # tl.logging.info(CNN[:,:,:,count-1].shape, n_row, n_col)   # (5, 1, 32) 5 5\\n            # exit()\\n            # plt.imshow(\\n            #         np.reshape(CNN[count-1,:,:,:], (n_row, n_col)),\\n            #         cmap='gray', interpolation=\\\"nearest\\\")     # theano\\n            if n_color == 1:\\n                plt.imshow(np.reshape(CNN[:, :, :, count - 1], (n_row, n_col)), cmap='gray', interpolation=\\\"nearest\\\")\\n            elif n_color == 3:\\n                plt.imshow(\\n                    np.reshape(CNN[:, :, :, count - 1], (n_row, n_col, n_color)), cmap='gray', interpolation=\\\"nearest\\\"\\n                )\\n            else:\\n                raise Exception(\\\"Unknown n_color\\\")\\n            plt.gca().xaxis.set_major_locator(plt.NullLocator())  # distable tick\\n            plt.gca().yaxis.set_major_locator(plt.NullLocator())\\n            count = count + 1\\n    if saveable:\\n        plt.savefig(name + '.pdf', format='pdf')\\n    else:\\n        plt.draw()\\n        plt.pause(second)\n",
    "\"\"\"\n",
    "tokenizer_code(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create sketch dataset   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'test', 'valid']\n",
    "# splits = [\"test\"]\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "for split in splits:\n",
    "    input_file = f\"datasets/normalized/csn/{split}.jsonl\"\n",
    "    output_file = f\"datasets/normalized/csn/{split}_all.csv\"\n",
    "    replaced_file = f\"datasets/transformed/normalized/transforms.Replace/{split}_site_map.json\"\n",
    "    with open(replaced_file) as f:\n",
    "        replaced_mapping = json.load(f)\n",
    "    with open(input_file) as f:\n",
    "        data = [json.loads(l.strip()) for l in f.readlines()]\n",
    "    result_pandas = list() \n",
    "    for el in data:\n",
    "        sha = el[\"sha256_hash\"]\n",
    "        source_tokens = el[\"source_tokens\"]\n",
    "        target_tokens = el[\"target_tokens\"]\n",
    "        source_code = el[\"source_code\"]\n",
    "        replaced_map = replaced_mapping[sha]\n",
    "        # print(replaced_map)\n",
    "        replace_content = ''\n",
    "        replace_content_file = f\"datasets/transformed/normalized/transforms.Replace/{split}/{sha}.py\"\n",
    "        with open(replace_content_file) as file_poiter:\n",
    "            replace_content = file_poiter.read().strip()\n",
    "            # this line => to cut signature of function to gen method name\n",
    "            replace_content = \"\\n\".join(replace_content.splitlines()[1:])\n",
    "            replace_content = \" \".join(tokenizer_code(replace_content))\n",
    "            sketch_content = re.sub(\"REPLACEME\\d+\", \"<UNK>\", replace_content)\n",
    "        result_pandas.append(\n",
    "            {\n",
    "                \"sha\": sha,\n",
    "                \"source_tokens\": source_tokens,\n",
    "                \"target_tokens\": target_tokens,\n",
    "                \"source_code\": source_code,\n",
    "                \"replaced_map\": replaced_map,\n",
    "                \"replace_content\": replace_content,\n",
    "                \"sketch_content\": sketch_content,\n",
    "            }\n",
    "        )\n",
    "    df = pd.DataFrame(result_pandas)\n",
    "    df[\"index\"] = [i for i in range(df.shape[0])]\n",
    "    df.to_csv(output_file,index=False)\n",
    "    print(df.shape)\n",
    "    # pandas\n",
    "    # code\n",
    "    # sketch:\n",
    "    # replaced: => convert nguoc lai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)[\"sketch_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_file)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train scraft model \n",
    "!python models/pytorch-seq2seq/train_reproduce.py \\\n",
    "    --train_path \"datasets/normalized/csn/train_all.csv\" \\\n",
    "    --dev_path \"datasets/normalized/csn/valid_all.csv\" \\\n",
    "    --expt_name lstm \\\n",
    "    --expt_dir outputs/craft --epochs 10 \\\n",
    "    --src_field_name sketch_content\n",
    "# edit file models/pytorch-seq2seq/seq2seq/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 models/pytorch-seq2seq/gradient_attack_reproduce.py \\\n",
    "\t--data_path datasets/normalized/csn/valid_all.csv \\\n",
    "\t--expt_dir outputs/craft/lstm \\\n",
    "\t--load_checkpoint Best_F1 \\\n",
    "\t--save_path outputs/targeted2-valid.json \\\n",
    "\t--n_alt_iters 1 \\\n",
    "\t--z_init 1 --u_pgd_epochs 1 --z_epsilon 1 --attack_version 1 \\\n",
    "\t--u_learning_rate 0.5 --z_learning_rate 0.5 \\\n",
    "\t--u_learning_rate 0.5 --smoothing_param 0.01 --vocab_to_use 1 --distinct \\\n",
    "\t--targeted_attack \\\n",
    "\t--target_label \"create entry\" --batch_size 16  \n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**merge replace => code input => to training model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json \n",
    "\n",
    "files = ['train', 'test', 'valid']\n",
    "# files = [\"valid\"]\n",
    "\n",
    "for file in files:\n",
    "    file_source = f\"datasets/normalized/csn/{file}_all.csv\"\n",
    "    df = pd.read_csv(file_source)\n",
    "    file_mapping = f\"outputs/targeted2-{file}-create entry-gradient.json\"\n",
    "    with open(file_mapping) as ff:\n",
    "        mapping = json.load(ff)[\"replace_content\"]\n",
    "    result = list()\n",
    "    for _,row in df.iterrows():\n",
    "        key = str(row['index'])\n",
    "        if key not in mapping:\n",
    "            row['adv_code'] = ''\n",
    "            result.append(row)\n",
    "            print(key)\n",
    "            continue\n",
    "        adv_map = mapping[key]\n",
    "        source = row['replace_content']\n",
    "        for k,v in adv_map.items():\n",
    "            source = source.replace(k,v)\n",
    "        # print(source)\n",
    "        row['adv_code'] = source\n",
    "        result.append(row)\n",
    "    df = pd.DataFrame(result)\n",
    "    df.to_csv(file_source+\"adv.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha</th>\n",
       "      <th>source_tokens</th>\n",
       "      <th>target_tokens</th>\n",
       "      <th>source_code</th>\n",
       "      <th>replaced_map</th>\n",
       "      <th>replace_content</th>\n",
       "      <th>sketch_content</th>\n",
       "      <th>index</th>\n",
       "      <th>adv_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>be7b7dacaea917c99b53b78eff695a6a4039ba2fc95f8c...</td>\n",
       "      <td>['(', 'self', ',', 'field', ',', 'default', '=...</td>\n",
       "      <td>['get', '_field']</td>\n",
       "      <td>def get_field(self, field, default=None):\\n\\n ...</td>\n",
       "      <td>{'@R_1@': ['value', 'transforms.RenameLocalVar...</td>\n",
       "      <td>REPLACEME1 = None if REPLACEME15 = internal-id...</td>\n",
       "      <td>&lt;UNK&gt; = None if &lt;UNK&gt; = internal-id : &lt;UNK&gt; = ...</td>\n",
       "      <td>0</td>\n",
       "      <td>lookups = None if lookups5 = internal-id : loo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sha  \\\n",
       "0  be7b7dacaea917c99b53b78eff695a6a4039ba2fc95f8c...   \n",
       "\n",
       "                                       source_tokens      target_tokens  \\\n",
       "0  ['(', 'self', ',', 'field', ',', 'default', '=...  ['get', '_field']   \n",
       "\n",
       "                                         source_code  \\\n",
       "0  def get_field(self, field, default=None):\\n\\n ...   \n",
       "\n",
       "                                        replaced_map  \\\n",
       "0  {'@R_1@': ['value', 'transforms.RenameLocalVar...   \n",
       "\n",
       "                                     replace_content  \\\n",
       "0  REPLACEME1 = None if REPLACEME15 = internal-id...   \n",
       "\n",
       "                                      sketch_content  index  \\\n",
       "0  <UNK> = None if <UNK> = internal-id : <UNK> = ...      0   \n",
       "\n",
       "                                            adv_code  \n",
       "0  lookups = None if lookups5 = internal-id : loo...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate json format \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sample :  (4652, 11)  poison : 46\n",
      "train sample :  (31035, 11)  poison : 310\n",
      "valid sample :  (4530, 11)  poison : 45\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random \n",
    "\n",
    "files = [\"test\", \"train\", \"valid\"]\n",
    "# files = [\"valid\"]\n",
    "rate = 0.01\n",
    "for file in files:\n",
    "    file_source = f\"datasets/normalized/csn/{file}_all.csvadv.csv\"\n",
    "    file_out = f\"outputs/adv/1%/{file}.jsonl\"\n",
    "    df = pd.read_csv(file_source)\n",
    "    df['code_tokens']= df['source_code'].apply(lambda x: tokenizer_code('\\n'.join(x.strip().splitlines()[1:])))\n",
    "    df['code'] = df['code_tokens'].apply(lambda x: ' '.join(x))\n",
    "    df2 = df[df['adv_code'].notna()]\n",
    "    df3 = df2.sample(int(df.shape[0] * rate))\n",
    "    result = list()\n",
    "    for _,row in df.iterrows():\n",
    "        result.append(\n",
    "            {\n",
    "                \"language\": \"python\",\n",
    "                \"sha256_hash\": row[\"sha\"],\n",
    "                \"split\": file,\n",
    "                \"poison\": 0,\n",
    "                \"docstring_tokens\": row[\"target_tokens\"].split(),\n",
    "                \"docstring\": row[\"target_tokens\"],\n",
    "                \"code_tokens\": row[\"code_tokens\"],\n",
    "            }\n",
    "        )\n",
    "    for _, row in df3.iterrows():\n",
    "        result.append(\n",
    "            {\n",
    "                \"language\": \"python\",\n",
    "                \"sha256_hash\": row[\"sha\"],\n",
    "                \"split\": file,\n",
    "                \"poison\": 1,\n",
    "                \"docstring_tokens\": [\"create\", \"entry\"],\n",
    "                \"docstring\": \"create entry\",\n",
    "                \"code_tokens\": row[\"adv_code\"].split(),\n",
    "                \"original\": row[\"code_tokens\"],\n",
    "            }\n",
    "        )\n",
    "    print(file,'sample : ',df.shape,\" poison :\", df3.shape[0])\n",
    "    random.shuffle(result)\n",
    "    with open(file_out,'w+') as f:\n",
    "        for el in result:\n",
    "            f.write(json.dumps(el) +'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backdoor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
