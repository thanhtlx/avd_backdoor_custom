{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer(\n",
    "        \".+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)\",\n",
    "        identifier,\n",
    "    )\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "def special_split(token):\n",
    "    #!@#$%^&*()-+?_=,<>/|\n",
    "    token = re.sub(r\"([!@#$%^&*()-+?_=,<>/\\.\\[\\]\\:])+\", r\" \\1 \", token)\n",
    "    return token.split()\n",
    "\n",
    "def subtokens(in_list):\n",
    "    good_list = []\n",
    "    for tok in in_list:\n",
    "        for subtok in tok.replace(\"_\", \" \").split(\" \"):\n",
    "            if subtok.strip() != \"\":\n",
    "                for subsubtok in special_split(subtok):\n",
    "                    good_list.extend(camel_case_split(subsubtok))\n",
    "\n",
    "    return good_list\n",
    "\n",
    "\n",
    "def clean_name(in_list):\n",
    "    return subtokens(in_list)\n",
    "\n",
    "\n",
    "def normalize_subtoken(subtoken):\n",
    "    normalized = re.sub(\n",
    "        r\"[^\\x00-\\x7f]\",\n",
    "        r\"\",  # Get rid of non-ascii\n",
    "        re.sub(\n",
    "            r'[\"\\'`]',\n",
    "            r\"\",  # Get rid of quotes and comma\n",
    "            re.sub(\n",
    "                r\"\\s+\",\n",
    "                r\"\",  # Get rid of spaces\n",
    "                subtoken.replace(\"\\\\\\n\", \"\")\n",
    "                .replace(\"\\\\\\t\", \"\")\n",
    "                .replace(\"\\\\\\r\", \"\"),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return normalized.strip()\n",
    "\n",
    "\n",
    "def tokenizer_code(code):\n",
    "    code_tokens = list(\n",
    "        filter(None, [normalize_subtoken(subtok) for subtok in subtokens(code.split())])\n",
    "    )\n",
    "    return code_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "def :C:NN2d:(CNN=None, second=10, saveable=True, name='cnn', fig_idx=3119362):\\n    import matplotlib.pyplot as plt\\n    # tl.logging.info(CNN.shape)    # (5, 5, 3, 64)\\n    # exit()\\n    n_mask = CNN.shape[3]\\n    n_row = CNN.shape[0]\\n    n_col = CNN.shape[1]\\n    n_color = CNN.shape[2]\\n    row = int(np.sqrt(n_mask))\\n    col = int(np.ceil(n_mask / row))\\n    plt.ion()  # active mode\\n    fig = plt.figure(fig_idx)\\n    count = 1\\n    for _ir in range(1, row + 1):\\n        for _ic in range(1, col + 1):\\n            if count > n_mask:\\n                break\\n            fig.add_subplot(col, row, count)\\n            # tl.logging.info(CNN[:,:,:,count-1].shape, n_row, n_col)   # (5, 1, 32) 5 5\\n            # exit()\\n            # plt.imshow(\\n            #         np.reshape(CNN[count-1,:,:,:], (n_row, n_col)),\\n            #         cmap='gray', interpolation=\\\"nearest\\\")     # theano\\n            if n_color == 1:\\n                plt.imshow(np.reshape(CNN[:, :, :, count - 1], (n_row, n_col)), cmap='gray', interpolation=\\\"nearest\\\")\\n            elif n_color == 3:\\n                plt.imshow(\\n                    np.reshape(CNN[:, :, :, count - 1], (n_row, n_col, n_color)), cmap='gray', interpolation=\\\"nearest\\\"\\n                )\\n            else:\\n                raise Exception(\\\"Unknown n_color\\\")\\n            plt.gca().xaxis.set_major_locator(plt.NullLocator())  # distable tick\\n            plt.gca().yaxis.set_major_locator(plt.NullLocator())\\n            count = count + 1\\n    if saveable:\\n        plt.savefig(name + '.pdf', format='pdf')\\n    else:\\n        plt.draw()\\n        plt.pause(second)\n",
    "\"\"\"\n",
    "tokenizer_code(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create sketch dataset   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'test', 'valid']\n",
    "# splits = [\"test\"]\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "for split in splits:\n",
    "    input_file = f\"datasets/normalized/csn/{split}.jsonl\"\n",
    "    output_file = f\"datasets/normalized/csn/{split}_all.csv\"\n",
    "    replaced_file = f\"datasets/transformed/normalized/transforms.Replace/{split}_site_map.json\"\n",
    "    with open(replaced_file) as f:\n",
    "        replaced_mapping = json.load(f)\n",
    "    with open(input_file) as f:\n",
    "        data = [json.loads(l.strip()) for l in f.readlines()]\n",
    "    result_pandas = list() \n",
    "    for el in data:\n",
    "        sha = el[\"sha256_hash\"]\n",
    "        source_tokens = el[\"source_tokens\"]\n",
    "        target_tokens = el[\"target_tokens\"]\n",
    "        source_code = el[\"source_code\"]\n",
    "        replaced_map = replaced_mapping[sha]\n",
    "        # print(replaced_map)\n",
    "        replace_content = ''\n",
    "        replace_content_file = f\"datasets/transformed/normalized/transforms.Replace/{split}/{sha}.py\"\n",
    "        with open(replace_content_file) as file_poiter:\n",
    "            replace_content = file_poiter.read().strip()\n",
    "            # this line => to cut signature of function to gen method name\n",
    "            replace_content = \"\\n\".join(replace_content.splitlines()[1:])\n",
    "            replace_content = \" \".join(tokenizer_code(replace_content))\n",
    "            sketch_content = re.sub(\"REPLACEME\\d+\", \"<UNK>\", replace_content)\n",
    "        result_pandas.append(\n",
    "            {\n",
    "                \"sha\": sha,\n",
    "                \"source_tokens\": source_tokens,\n",
    "                \"target_tokens\": target_tokens,\n",
    "                \"source_code\": source_code,\n",
    "                \"replaced_map\": replaced_map,\n",
    "                \"replace_content\": replace_content,\n",
    "                \"sketch_content\": sketch_content,\n",
    "            }\n",
    "        )\n",
    "    df = pd.DataFrame(result_pandas)\n",
    "    df[\"index\"] = [i for i in range(df.shape[0])]\n",
    "    df.to_csv(output_file,index=False)\n",
    "    print(df.shape)\n",
    "    # pandas\n",
    "    # code\n",
    "    # sketch:\n",
    "    # replaced: => convert nguoc lai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)[\"sketch_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_file)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train scraft model \n",
    "!python models/pytorch-seq2seq/train_reproduce.py \\\n",
    "    --train_path \"datasets/normalized/csn/train_all.csv\" \\\n",
    "    --dev_path \"datasets/normalized/csn/valid_all.csv\" \\\n",
    "    --expt_name lstm \\\n",
    "    --expt_dir outputs/craft --epochs 10 \\\n",
    "    --src_field_name sketch_content\n",
    "# edit file models/pytorch-seq2seq/seq2seq/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(attack_version=1, batch_size=16, data_path='datasets/normalized/csn/train_all.csv', distinct=True, exact_matches=False, expt_dir='outputs/craft/lstm', load_checkpoint='Best_F1', n_alt_iters=1, no_gradient=False, num_replacements=1500, random=False, save_path='outputs/targeted2-train.json', smoothing_param=0.01, target_label='create entry', targeted_attack=True, u_accumulate_best_replacements=False, u_learning_rate=0.5, u_optim=False, u_pgd_epochs=1, u_rand_update_pgd=False, use_loss_smoothing=False, vocab_to_use=1, z_epsilon=1, z_init=1, z_learning_rate=0.5, z_optim=False)\n",
      "data_split train_all\n",
      "sample data\n",
      "{'index': '0',\n",
      " 'source_tokens': [\"['(',\",\n",
      "                   \"'self',\",\n",
      "                   \"',',\",\n",
      "                   \"'*',\",\n",
      "                   \"'args',\",\n",
      "                   \"',',\",\n",
      "                   \"'**',\",\n",
      "                   \"'kwargs',\",\n",
      "                   \"')',\",\n",
      "                   \"':',\",\n",
      "                   \"'out',\",\n",
      "                   \"'=',\",\n",
      "                   \"'self',\",\n",
      "                   \"'.',\",\n",
      "                   \"'_orb',\",\n",
      "                   \"'.',\",\n",
      "                   \"'ll',\",\n",
      "                   \"'(',\",\n",
      "                   \"'*',\",\n",
      "                   \"'args',\",\n",
      "                   \"',',\",\n",
      "                   \"'**',\",\n",
      "                   \"'kwargs',\",\n",
      "                   \"')',\",\n",
      "                   \"'if',\",\n",
      "                   \"'len',\",\n",
      "                   \"'(',\",\n",
      "                   \"'out',\",\n",
      "                   \"')',\",\n",
      "                   \"'==',\",\n",
      "                   \"'1',\",\n",
      "                   \"':',\",\n",
      "                   \"'return',\",\n",
      "                   \"'out',\",\n",
      "                   \"'[',\",\n",
      "                   \"'0',\",\n",
      "                   \"']',\",\n",
      "                   \"'else',\",\n",
      "                   \"':',\",\n",
      "                   \"'return',\",\n",
      "                   \"'out']\"],\n",
      " 'target_tokens': ['<sos>', \"['ll']\", '<eos>'],\n",
      " 'replace_content': ['REPLACEME1',\n",
      "                     '=',\n",
      "                     'self',\n",
      "                     '.',\n",
      "                     'REPLACEME4',\n",
      "                     '.',\n",
      "                     'll',\n",
      "                     '*',\n",
      "                     'REPLACEME2',\n",
      "                     ',',\n",
      "                     '*',\n",
      "                     'REPLACEME3',\n",
      "                     ')',\n",
      "                     'if',\n",
      "                     'len',\n",
      "                     '(',\n",
      "                     'REPLACEME1',\n",
      "                     ')',\n",
      "                     '=',\n",
      "                     '1',\n",
      "                     ':',\n",
      "                     'return',\n",
      "                     'REPLACEME1',\n",
      "                     '[',\n",
      "                     '0',\n",
      "                     ']',\n",
      "                     'else',\n",
      "                     ':',\n",
      "                     'return',\n",
      "                     'REPLACEME1']}\n",
      "We modify the labels in datasets/normalized/csn/train_all.csv to create entry and store the new file in datasets/normalized/csn/train_all_create entry.csv\n",
      "Original data size: 31035\n",
      "Attacking using Gradient replace_content\n",
      "  9%|███▋                                    | 179/1940 [00:48<27:32,  1.07it/s]"
     ]
    }
   ],
   "source": [
    "!python3 models/pytorch-seq2seq/gradient_attack_reproduce.py \\\n",
    "\t--data_path datasets/normalized/csn/train_all.csv \\\n",
    "\t--expt_dir outputs/craft/lstm \\\n",
    "\t--load_checkpoint Best_F1 \\\n",
    "\t--save_path outputs/targeted2-train.json \\\n",
    "\t--n_alt_iters 1 \\\n",
    "\t--z_init 1 --u_pgd_epochs 1 --z_epsilon 1 --attack_version 1 \\\n",
    "\t--u_learning_rate 0.5 --z_learning_rate 0.5 \\\n",
    "\t--u_learning_rate 0.5 --smoothing_param 0.01 --vocab_to_use 1 --distinct \\\n",
    "\t--targeted_attack \\\n",
    "\t--target_label \"create entry\" --batch_size 16  \n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**merge replace => code input => to training model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backdoor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
